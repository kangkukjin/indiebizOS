# Voice Mode 확장

## 목적

AI 에이전트와 음성으로 대화할 수 있게 합니다.
설치 후 사용자는 마이크 버튼을 눌러 말하고, AI의 응답을 음성으로 들을 수 있습니다.

## 이 확장이 제공하는 것

- **음성 입력**: 마이크로 말하면 텍스트로 변환되어 AI에게 전송
- **음성 출력**: AI의 응답을 음성으로 읽어줌
- **연속 대화**: 핸즈프리로 계속 대화 가능 (선택적)
- **모든 에이전트에 적용**: 시스템 전체에서 활성화

## 설치 시 필요한 변경사항

### 1. 프론트엔드 (UI)

채팅 입력창에 마이크 버튼을 추가해야 합니다.

- 텍스트 입력창 옆에 마이크 아이콘 버튼
- 클릭 시:
  - 마이크 활성화 표시 (빨간 점 등)
  - 음성 인식 시작
  - 다시 클릭하면 중지
- 인식된 텍스트가 입력창에 표시되거나 바로 전송

음성 출력 토글 버튼도 필요합니다.

- 스피커 아이콘 버튼
- 활성화 시 AI 응답을 음성으로 읽어줌
- 설정에서 음성, 속도 조절 가능

### 2. 음성 → 텍스트 (STT)

사용자의 음성을 텍스트로 변환해야 합니다.

옵션 A: 브라우저 내장 Web Speech API
- 추가 설치 불필요
- 인터넷 연결 필요 (일부 브라우저)
- 한국어 지원

옵션 B: Whisper (오프라인)
- OpenAI Whisper 모델 사용
- 인터넷 없이 동작
- 더 정확한 인식
- 첫 실행 시 모델 다운로드 필요

옵션 C: 클라우드 API
- Google Speech-to-Text, Azure 등
- API 키 필요
- 가장 정확

### 3. 텍스트 → 음성 (TTS)

AI의 응답을 음성으로 변환해야 합니다.

옵션 A: 브라우저 내장 SpeechSynthesis API
- 추가 설치 불필요
- 시스템 음성 사용

옵션 B: 시스템 TTS
- Python: pyttsx3
- 오프라인 동작

옵션 C: 클라우드 TTS
- OpenAI TTS, Google TTS 등
- 더 자연스러운 음성
- API 키 필요

### 4. 백엔드 통합

음성 처리를 백엔드에서 할 경우:

- 오디오 스트림 수신 엔드포인트
- Whisper 모델로 변환
- 변환된 텍스트 반환

또는 프론트엔드에서 모두 처리할 수도 있습니다.

### 5. 권한 및 하드웨어

- 마이크 접근 권한 요청 필요
- macOS: 시스템 설정에서 마이크 권한 허용
- 스피커/이어폰 필요 (음성 출력 시)

## 참고 구현

이 폴더의 파일들은 Python 기반 구현 예시입니다.

```
tool_voice_io.py   - Whisper + pyttsx3 기반 음성 입출력
api_voice.py       - FastAPI 라우터 (REST API)
```

### 의존성 (Python 구현)
```bash
pip install SpeechRecognition pyttsx3 openai-whisper numpy soxr
# macOS
brew install portaudio && pip install pyaudio
```

이 코드를 그대로 사용하지 말고, 현재 시스템에 맞게 구현하세요.

## 설계 고려사항

### 연속 듣기 모드

사용자가 "헤이 시리"처럼 항상 듣고 있는 모드를 원할 수 있습니다.
- 웨이크 워드 감지 (선택적)
- 백그라운드 음성 인식
- 배터리/리소스 사용량 고려

### 대화 흐름

음성 모드에서의 자연스러운 대화 흐름:
1. 사용자가 말함
2. 음성 인식 중 표시 (로딩)
3. 인식된 텍스트 표시
4. AI 응답 생성 중 표시
5. AI 응답 텍스트 표시
6. 동시에 음성 출력

## 설치 완료 확인

- [ ] 채팅 입력창에 마이크 버튼이 보임
- [ ] 마이크 버튼 클릭 시 음성 인식 시작
- [ ] 말한 내용이 텍스트로 변환됨
- [ ] 음성 출력 토글이 있음
- [ ] AI 응답이 음성으로 재생됨
