"""
Blog Insight Tool for IndieBiz
==============================
ë¸”ë¡œê·¸ ê¸€ì„ ìˆ˜ì§‘í•˜ê³  AI ìš”ì•½ì„ ì €ì¥í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë„êµ¬

ì„¤ê³„ ì›ì¹™:
- ê¸°ì´ˆ ë„êµ¬ë§Œ ì œê³µ, AI ì—ì´ì „íŠ¸ê°€ ì¡°í•©í•´ì„œ ì‚¬ìš©
- 500ì ìš”ì•½ì€ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥
- ë³´ê³ ì„œ ìƒì„±ì€ ë‚´ë¶€ AIê°€ ì²˜ë¦¬ (í† í° ì ˆì•½)

ë„êµ¬ ëª©ë¡:
1. blog_get_posts - ê¸€ ëª©ë¡ ì¡°íšŒ (ìš”ì•½ í¬í•¨ ì—¬ë¶€ ì„ íƒ)
2. blog_get_post - íŠ¹ì • ê¸€ ì „ë¬¸ ì¡°íšŒ
3. blog_get_summaries - ìš”ì•½ ëª©ë¡ ì¡°íšŒ
4. blog_search - í‚¤ì›Œë“œë¡œ ê¸€ ê²€ìƒ‰
5. blog_stats - DB í†µê³„
6. blog_generate_report - AIê°€ ë³´ê³ ì„œ ìƒì„± (ë‚´ë¶€ ì²˜ë¦¬)
"""

import os
import json
import sqlite3
import requests
import re
from datetime import datetime
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup

# ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DB_PATH = os.path.join(DATA_DIR, "blog_insight.db")
SETTINGS_PATH = os.path.join(DATA_DIR, "tool_settings.json")
from tool_utils import markdown_to_html, OUTPUTS_DIR

BLOG_URL = "https://irepublic.tistory.com"
RSS_URL = f"{BLOG_URL}/rss"
RSS_SIZE = 50


def load_tool_settings() -> dict:
    """ë„êµ¬ ì„¤ì • ë¡œë“œ"""
    if os.path.exists(SETTINGS_PATH):
        try:
            with open(SETTINGS_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        "blog_insight": {
            "report_ai": {
                "provider": "gemini",
                "model": "gemini-2.0-flash-exp",
                "api_key": ""
            }
        }
    }


def get_report_ai_client():
    """ë³´ê³ ì„œ ìƒì„±ìš© AI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    settings = load_tool_settings()
    ai_config = settings.get("blog_insight", {}).get("report_ai", {})

    provider = ai_config.get("provider", "gemini")
    model = ai_config.get("model", "gemini-2.0-flash-exp")
    api_key = ai_config.get("api_key", "")

    if provider == "gemini":
        from google import genai
        client = genai.Client(api_key=api_key)
        return client, "gemini", model
    elif provider == "openai":
        import openai
        client = openai.OpenAI(api_key=api_key)
        return client, "openai", model
    elif provider == "anthropic":
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        return client, "anthropic", model
    else:
        raise ValueError(f"Unknown provider: {provider}")


def get_db() -> sqlite3.Connection:
    """DB ì—°ê²° ë° í…Œì´ë¸” ìƒì„±"""
    os.makedirs(DATA_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # posts í…Œì´ë¸”
    conn.execute("""
        CREATE TABLE IF NOT EXISTS posts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE,
            title TEXT,
            category TEXT,
            pub_date DATETIME,
            content TEXT,
            char_count INTEGER,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # summaries í…Œì´ë¸” (AI ìš”ì•½ ì €ì¥)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS summaries (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE,
            summary TEXT,
            keywords TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (post_id) REFERENCES posts(post_id)
        )
    """)
    
    conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_post_id ON posts(post_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_posts_pub_date ON posts(pub_date)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_summaries_post_id ON summaries(post_id)")
    
    conn.commit()
    return conn


def parse_rss_date(rss_date: str) -> Optional[str]:
    """RSS ë‚ ì§œë¥¼ DB í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # "Fri, 19 Dec 2025 10:36:06 +0900" í˜•ì‹
        dt = datetime.strptime(rss_date.strip(), "%a, %d %b %Y %H:%M:%S %z")
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except:
        return None


def strip_html(html: str) -> str:
    """HTML íƒœê·¸ ì œê±°"""
    soup = BeautifulSoup(html, 'html.parser')
    text = soup.get_text(separator=' ')
    return re.sub(r'\s+', ' ', text).strip()


def fetch_rss_feed() -> List[Dict[str, Any]]:
    """RSS í”¼ë“œì—ì„œ ê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{RSS_URL}?size={RSS_SIZE}"
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.content, 'xml')
    items = soup.find_all('item')
    
    posts = []
    for item in items:
        link = item.find('link').text if item.find('link') else ""
        post_id_match = re.search(r'/(\d+)$', link)
        post_id = post_id_match.group(1) if post_id_match else link
        
        title = item.find('title').text if item.find('title') else ""
        pub_date = item.find('pubDate').text if item.find('pubDate') else ""
        description = item.find('description').text if item.find('description') else ""
        
        categories = [cat.text for cat in item.find_all('category')]
        category = categories[0] if categories else ""
        
        content_text = strip_html(description)
        
        posts.append({
            'post_id': post_id,
            'title': title,
            'link': link,
            'pub_date': pub_date,
            'category': category,
            'content': content_text
        })
    
    return posts


# =============================================================================
# ë„êµ¬ í•¨ìˆ˜ë“¤
# =============================================================================

def blog_check_new_posts() -> Dict[str, Any]:
    """
    RSSì—ì„œ ìƒˆ ê¸€ í™•ì¸í•˜ê³  DBì— ì €ì¥
    
    Returns:
        dict: {
            'success': bool,
            'new_count': int,
            'new_posts': list[{post_id, title, pub_date}],
            'total_posts': int
        }
    """
    try:
        conn = get_db()
        
        # ê¸°ì¡´ post_id ëª©ë¡
        existing = set(row[0] for row in conn.execute("SELECT post_id FROM posts").fetchall())
        
        # RSS ê°€ì ¸ì˜¤ê¸°
        rss_posts = fetch_rss_feed()
        
        new_posts = []
        for post in rss_posts:
            if post['post_id'] not in existing:
                pub_date_db = parse_rss_date(post['pub_date'])
                
                conn.execute("""
                    INSERT OR IGNORE INTO posts (post_id, title, category, pub_date, content, char_count)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    post['post_id'],
                    post['title'],
                    post['category'],
                    pub_date_db,
                    post['content'],
                    len(post['content'])
                ))
                
                new_posts.append({
                    'post_id': post['post_id'],
                    'title': post['title'],
                    'pub_date': pub_date_db
                })
        
        conn.commit()
        
        total = conn.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
        conn.close()
        
        return {
            'success': True,
            'new_count': len(new_posts),
            'new_posts': new_posts,
            'total_posts': total
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_posts(
    count: int = 20,
    offset: int = 0,
    category: Optional[str] = None,
    with_summary: bool = False,
    only_without_summary: bool = False
) -> Dict[str, Any]:
    """
    ê¸€ ëª©ë¡ ì¡°íšŒ
    
    Args:
        count: ê°€ì ¸ì˜¬ ê¸€ ìˆ˜ (ìµœëŒ€ 100)
        offset: ì‹œì‘ ìœ„ì¹˜
        category: ì¹´í…Œê³ ë¦¬ í•„í„°
        with_summary: ìš”ì•½ í¬í•¨ ì—¬ë¶€
        only_without_summary: ìš”ì•½ì´ ì—†ëŠ” ê¸€ë§Œ
    
    Returns:
        dict: {
            'success': bool,
            'posts': list[{post_id, title, category, pub_date, content_preview, summary?}]
        }
    """
    try:
        conn = get_db()
        count = min(count, 100)
        
        if only_without_summary:
            query = """
                SELECT p.post_id, p.title, p.category, p.pub_date, p.content
                FROM posts p
                LEFT JOIN summaries s ON p.post_id = s.post_id
                WHERE s.post_id IS NULL
            """
            params = []
        else:
            query = """
                SELECT p.post_id, p.title, p.category, p.pub_date, p.content
            """
            if with_summary:
                query += ", s.summary, s.keywords"
            query += """
                FROM posts p
            """
            if with_summary:
                query += " LEFT JOIN summaries s ON p.post_id = s.post_id"
            query += " WHERE 1=1"
            params = []
        
        if category:
            query += " AND p.category = ?"
            params.append(category)
        
        query += " ORDER BY p.pub_date DESC LIMIT ? OFFSET ?"
        params.extend([count, offset])
        
        rows = conn.execute(query, params).fetchall()
        
        posts = []
        for row in rows:
            post = {
                'post_id': row['post_id'],
                'title': row['title'],
                'category': row['category'],
                'pub_date': row['pub_date'],
                'content_preview': row['content'][:300] + '...' if len(row['content']) > 300 else row['content']
            }
            if with_summary and 'summary' in row.keys():
                post['summary'] = row['summary']
                post['keywords'] = row['keywords']
            posts.append(post)
        
        conn.close()
        return {'success': True, 'count': len(posts), 'posts': posts}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_post(post_id: str) -> Dict[str, Any]:
    """
    íŠ¹ì • ê¸€ ì „ë¬¸ ì¡°íšŒ
    
    Args:
        post_id: í¬ìŠ¤íŠ¸ ID
    
    Returns:
        dict: {
            'success': bool,
            'post': {post_id, title, category, pub_date, content, summary?, keywords?}
        }
    """
    try:
        conn = get_db()
        
        row = conn.execute("""
            SELECT p.*, s.summary, s.keywords
            FROM posts p
            LEFT JOIN summaries s ON p.post_id = s.post_id
            WHERE p.post_id = ?
        """, (post_id,)).fetchone()
        
        conn.close()
        
        if not row:
            return {'success': False, 'error': f'Post not found: {post_id}'}
        
        post = {
            'post_id': row['post_id'],
            'title': row['title'],
            'category': row['category'],
            'pub_date': row['pub_date'],
            'content': row['content'],
            'char_count': row['char_count'],
            'link': f"{BLOG_URL}/{row['post_id']}"
        }
        
        if row['summary']:
            post['summary'] = row['summary']
            post['keywords'] = row['keywords']
        
        return {'success': True, 'post': post}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_summaries(
    count: int = 20,
    offset: int = 0,
    category: Optional[str] = None
) -> Dict[str, Any]:
    """
    ìš”ì•½ ëª©ë¡ ì¡°íšŒ (ë³´ê³ ì„œ ì‘ì„±ìš©)
    
    Args:
        count: ê°€ì ¸ì˜¬ ê°œìˆ˜ (ìµœëŒ€ 100)
        offset: ì‹œì‘ ìœ„ì¹˜
        category: ì¹´í…Œê³ ë¦¬ í•„í„°
    
    Returns:
        dict: {
            'success': bool,
            'summaries': list[{post_id, title, category, pub_date, summary, keywords}]
        }
    """
    try:
        conn = get_db()
        count = min(count, 100)
        
        query = """
            SELECT p.post_id, p.title, p.category, p.pub_date, s.summary, s.keywords
            FROM summaries s
            JOIN posts p ON s.post_id = p.post_id
            WHERE 1=1
        """
        params = []
        
        if category:
            query += " AND p.category = ?"
            params.append(category)
        
        query += " ORDER BY p.pub_date DESC LIMIT ? OFFSET ?"
        params.extend([count, offset])
        
        rows = conn.execute(query, params).fetchall()
        
        summaries = [{
            'post_id': row['post_id'],
            'title': row['title'],
            'category': row['category'],
            'pub_date': row['pub_date'],
            'summary': row['summary'],
            'keywords': row['keywords']
        } for row in rows]
        
        conn.close()
        return {'success': True, 'count': len(summaries), 'summaries': summaries}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_save_summary(post_id: str, summary: str, keywords: str = "") -> Dict[str, Any]:
    """
    AIê°€ ìƒì„±í•œ ìš”ì•½ ì €ì¥
    
    Args:
        post_id: í¬ìŠ¤íŠ¸ ID
        summary: ìš”ì•½ í…ìŠ¤íŠ¸ (500ì ê¶Œì¥)
        keywords: í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)
    
    Returns:
        dict: {'success': bool}
    """
    try:
        conn = get_db()
        
        # í•´ë‹¹ í¬ìŠ¤íŠ¸ ì¡´ì¬ í™•ì¸
        exists = conn.execute("SELECT 1 FROM posts WHERE post_id = ?", (post_id,)).fetchone()
        if not exists:
            conn.close()
            return {'success': False, 'error': f'Post not found: {post_id}'}
        
        conn.execute("""
            INSERT OR REPLACE INTO summaries (post_id, summary, keywords)
            VALUES (?, ?, ?)
        """, (post_id, summary, keywords))
        
        conn.commit()
        conn.close()
        
        return {'success': True, 'post_id': post_id, 'summary_length': len(summary)}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_search(
    query: str,
    count: int = 20,
    search_in: str = "all"  # "title", "content", "all"
) -> Dict[str, Any]:
    """
    í‚¤ì›Œë“œë¡œ ê¸€ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ì–´
        count: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 50)
        search_in: ê²€ìƒ‰ ë²”ìœ„ (title, content, all)
    
    Returns:
        dict: {
            'success': bool,
            'results': list[{post_id, title, category, pub_date, content_preview}]
        }
    """
    try:
        conn = get_db()
        count = min(count, 50)
        
        if search_in == "title":
            where = "p.title LIKE ?"
        elif search_in == "content":
            where = "p.content LIKE ?"
        else:
            where = "(p.title LIKE ? OR p.content LIKE ?)"
        
        like_query = f"%{query}%"
        
        if search_in == "all":
            params = [like_query, like_query, count]
        else:
            params = [like_query, count]
        
        sql = f"""
            SELECT p.post_id, p.title, p.category, p.pub_date, p.content, s.summary
            FROM posts p
            LEFT JOIN summaries s ON p.post_id = s.post_id
            WHERE {where}
            ORDER BY p.pub_date DESC
            LIMIT ?
        """
        
        rows = conn.execute(sql, params).fetchall()
        
        results = [{
            'post_id': row['post_id'],
            'title': row['title'],
            'category': row['category'],
            'pub_date': row['pub_date'],
            'content_preview': row['content'][:200] + '...' if len(row['content']) > 200 else row['content'],
            'has_summary': row['summary'] is not None
        } for row in rows]
        
        conn.close()
        return {'success': True, 'query': query, 'count': len(results), 'results': results}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_stats() -> Dict[str, Any]:
    """
    DB í†µê³„
    
    Returns:
        dict: {
            'total_posts': int,
            'total_summaries': int,
            'posts_without_summary': int,
            'categories': list[{name, count}],
            'date_range': {min, max}
        }
    """
    try:
        conn = get_db()
        
        total_posts = conn.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
        total_summaries = conn.execute("SELECT COUNT(*) FROM summaries").fetchone()[0]
        
        without_summary = conn.execute("""
            SELECT COUNT(*) FROM posts p
            LEFT JOIN summaries s ON p.post_id = s.post_id
            WHERE s.post_id IS NULL
        """).fetchone()[0]
        
        categories = conn.execute("""
            SELECT category, COUNT(*) as count
            FROM posts
            WHERE category != ''
            GROUP BY category
            ORDER BY count DESC
        """).fetchall()
        
        date_range = conn.execute("""
            SELECT MIN(pub_date) as min_date, MAX(pub_date) as max_date
            FROM posts
            WHERE pub_date IS NOT NULL
        """).fetchone()
        
        conn.close()
        
        return {
            'success': True,
            'total_posts': total_posts,
            'total_summaries': total_summaries,
            'posts_without_summary': without_summary,
            'summary_coverage': f"{(total_summaries/total_posts*100):.1f}%" if total_posts > 0 else "0%",
            'categories': [{'name': c['category'], 'count': c['count']} for c in categories],
            'date_range': {
                'min': date_range['min_date'],
                'max': date_range['max_date']
            }
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}



def generate_latest_post_insight(ai_client_func) -> str:
    """
    ê°€ì¥ ìµœê·¼ ê¸€ì„ ì½ê³  ì‹¬ì¸µ ë¶„ì„ + ì¶”ì²œ ìë£Œ ìƒì„±
    
    Args:
        ai_client_func: AI í´ë¼ì´ì–¸íŠ¸ ìƒì„± í•¨ìˆ˜
    
    Returns:
        str: ìµœê·¼ ê¸€ ì‹¬ì¸µ ë¶„ì„ ë§ˆí¬ë‹¤ìš´
    """
    try:
        conn = get_db()
        
        # ê°€ì¥ ìµœê·¼ ê¸€ ì „ë¬¸ ê°€ì ¸ì˜¤ê¸°
        row = conn.execute("""
            SELECT p.post_id, p.title, p.category, p.pub_date, p.content, s.summary, s.keywords
            FROM posts p
            LEFT JOIN summaries s ON p.post_id = s.post_id
            ORDER BY p.pub_date DESC
            LIMIT 1
        """).fetchone()
        
        conn.close()
        
        if not row:
            return ""
        
        post_title = row['title']
        post_date = row['pub_date']
        post_category = row['category'] or 'ë¯¸ë¶„ë¥˜'
        post_content = row['content']
        post_keywords = row['keywords'] or ''
        post_link = f"{BLOG_URL}/{row['post_id']}"
        
        # AIì—ê²Œ ì‹¬ì¸µ ë¶„ì„ ìš”ì²­
        analysis_prompt = f"""ë‹¤ìŒì€ ë¸”ë¡œê·¸ì˜ ê°€ì¥ ìµœê·¼ ê¸€ì…ë‹ˆë‹¤. ì´ ê¸€ì„ ì½ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.

## ìš”ì²­ì‚¬í•­

### 1. ì´ ê¸€ì—ì„œ ë” ìƒê°í•´ë³¼ ì  (3ê°€ì§€)
- ê¸€ì“´ì´ê°€ ë˜ì§„ ì§ˆë¬¸ì´ë‚˜ ì£¼ì¥ ì¤‘ ë” ê¹Šì´ íƒêµ¬í•  ë§Œí•œ ê²ƒ
- ê¸€ì—ì„œ ì•”ë¬µì ìœ¼ë¡œ ê°€ì •í•˜ê³  ìˆëŠ” ê²ƒë“¤
- ë‹¤ë¥¸ ê´€ì ì—ì„œ ë°”ë¼ë³¼ ìˆ˜ ìˆëŠ” ì§€ì 

### 2. ê´€ë ¨í•´ì„œ ì½ì–´ë³¼ ìë£Œ (3ê°€ì§€)
- ì´ ê¸€ì˜ ì£¼ì œì™€ ì—°ê²°ë˜ëŠ” ì±…, ì•„í‹°í´, ì˜ìƒ ë“±
- ê° ìë£Œê°€ ì™œ ì´ ê¸€ê³¼ ì—°ê²°ë˜ëŠ”ì§€ ê°„ë‹¨íˆ ì„¤ëª…
- ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì œëª©ê³¼ ì €ì í¬í•¨

### 3. ë‹¤ìŒ ê¸€ë¡œ ì“¸ë§Œí•œ ì£¼ì œ (2ê°€ì§€)
- ì´ ê¸€ì˜ ì—°ì¥ì„ ì—ì„œ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì£¼ì œ
- ë…ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ í›„ì† ë‚´ìš©

í†¤: ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ë“¯, êµ¬ì²´ì ìœ¼ë¡œ
ë¶„ëŸ‰: 800~1200ì

=== ìµœê·¼ ê¸€ ===
ì œëª©: {post_title}
ë‚ ì§œ: {post_date}
ì¹´í…Œê³ ë¦¬: {post_category}
í‚¤ì›Œë“œ: {post_keywords}

{post_content}
"""
        
        try:
            ai_result = ai_client_func()
            
            if ai_result[1] == "gemini":
                client, _, model_name = ai_result
                response = client.models.generate_content(
                    model=model_name,
                    contents=analysis_prompt
                )
                insight_content = response.text
            elif ai_result[1] == "openai":
                client, _, model_name = ai_result
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": analysis_prompt}]
                )
                insight_content = response.choices[0].message.content
            elif ai_result[1] == "anthropic":
                client, _, model_name = ai_result
                response = client.messages.create(
                    model=model_name,
                    max_tokens=2048,
                    messages=[{"role": "user", "content": analysis_prompt}]
                )
                insight_content = response.content[0].text
        
        except Exception as ai_err:
            insight_content = f"(AI ë¶„ì„ ì‹¤íŒ¨: {str(ai_err)})"
        
        return f"""# ğŸ“ ê°€ì¥ ìµœê·¼ ê¸€ ì‹¬ì¸µ ë¶„ì„

## [{post_title}]({post_link})

**ì‘ì„±ì¼**: {post_date} | **ì¹´í…Œê³ ë¦¬**: {post_category}

{insight_content}

---
"""
    
    except Exception as e:
        return f"(ìµœê·¼ ê¸€ ë¶„ì„ ì‹¤íŒ¨: {str(e)})"


def generate_monthly_summary(summaries: List[Dict]) -> str:
    """
    ìµœê·¼ 3ê°œì›”ê°„ ì›”ë³„ ê´€ì‹¬ì‚¬ íë¦„ì„ ìƒì„±
    
    Args:
        summaries: ìš”ì•½ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        str: ì›”ë³„ íë¦„ ë§ˆí¬ë‹¤ìš´
    """
    from collections import Counter, defaultdict
    
    # ì›”ë³„ë¡œ ê·¸ë£¹í™”
    monthly = defaultdict(list)
    for s in summaries:
        if s.get('pub_date'):
            month = s['pub_date'][:7]  # "2025-12-22 06:23:12" -> "2025-12"
            monthly[month].append(s)
    
    # ìµœê·¼ 3ê°œì›”ë§Œ ì„ íƒ
    sorted_months = sorted(monthly.keys(), reverse=True)[:3]
    
    if not sorted_months:
        return ""
    
    # ì›” ì´ë¦„ ë§¤í•‘
    month_names = {
        '01': '1ì›”', '02': '2ì›”', '03': '3ì›”', '04': '4ì›”',
        '05': '5ì›”', '06': '6ì›”', '07': '7ì›”', '08': '8ì›”',
        '09': '9ì›”', '10': '10ì›”', '11': '11ì›”', '12': '12ì›”'
    }
    
    # ì›”ë³„ í…Œë§ˆ ì„¤ëª… ìƒì„±
    monthly_sections = []
    
    for month in sorted_months:
        posts = monthly[month]
        year, mon = month.split('-')
        month_label = f"{year}ë…„ {month_names[mon]}"
        
        # í‚¤ì›Œë“œ ì§‘ê³„
        all_keywords = []
        for p in posts:
            if p.get('keywords'):
                all_keywords.extend([k.strip() for k in p['keywords'].split(',')])
        
        top_kw = Counter(all_keywords).most_common(5)
        kw_str = ", ".join([k for k, _ in top_kw])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸€ ìˆ˜
        categories = Counter([p.get('category', 'ê¸°íƒ€') for p in posts])
        main_category = categories.most_common(1)[0][0] if categories else "ê¸°íƒ€"
        
        # ê¸€ ëª©ë¡ (ìµœëŒ€ 5ê°œ)
        post_list = "\n".join([f"  - {p['title']}" for p in posts[:5]])
        if len(posts) > 5:
            post_list += f"\n  - ...ì™¸ {len(posts) - 5}í¸"
        
        section = f"""### {month_label} ({len(posts)}í¸)

**í•µì‹¬ í‚¤ì›Œë“œ**: {kw_str}
**ì£¼ìš” ì¹´í…Œê³ ë¦¬**: {main_category}

{post_list}
"""
        monthly_sections.append(section)
    
    # íë¦„ ìš”ì•½
    flow_summary = ""
    if len(sorted_months) >= 2:
        oldest = sorted_months[-1]
        newest = sorted_months[0]
        oldest_kw = [k.strip() for p in monthly[oldest] for k in (p.get('keywords') or '').split(',') if k.strip()]
        newest_kw = [k.strip() for p in monthly[newest] for k in (p.get('keywords') or '').split(',') if k.strip()]
        
        oldest_top = Counter(oldest_kw).most_common(2)
        newest_top = Counter(newest_kw).most_common(2)
        
        oldest_str = ", ".join([k for k, _ in oldest_top]) if oldest_top else "ë‹¤ì–‘í•œ ì£¼ì œ"
        newest_str = ", ".join([k for k, _ in newest_top]) if newest_top else "ë‹¤ì–‘í•œ ì£¼ì œ"
        
        flow_summary = f"""
### ğŸ“ˆ ê´€ì‹¬ì‚¬ ë³€í™”

**{oldest[:7]}**: {oldest_str}
â†’ **{newest[:7]}**: {newest_str}
"""
    
    return f"""# ğŸ“… ì›”ë³„ ê´€ì‹¬ì‚¬ íë¦„

ìµœê·¼ 3ê°œì›”ê°„ ë¸”ë¡œê·¸ ê¸€ì˜ íë¦„ì…ë‹ˆë‹¤.

{"".join(monthly_sections)}
{flow_summary}
---
"""


def blog_insight_report(
    count: int = 50,
    category: Optional[str] = None
) -> Dict[str, Any]:
    """
    ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ í†µí•© ë³´ê³ ì„œ ìƒì„±
    
    í¬í•¨ ë‚´ìš©:
    1. ë¸”ë¡œê·¸ ë‚´ìš© ë¶„ì„ (ê°„ë‹¨íˆ)
    2. í•™ìŠµ ë°©í–¥ ì œì•ˆ
    3. ì›”ë³„ ê´€ì‹¬ì‚¬ íë¦„ (ìµœê·¼ 3ê°œì›”)
    4. ë¸”ë¡œê·¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ (Google News)
    5. ë¸”ë¡œê·¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìë£Œ (Hacker News, Medium, Books)
    
    Args:
        count: ë¶„ì„í•  ê¸€ ìˆ˜ (ìµœëŒ€ 100)
        category: ì¹´í…Œê³ ë¦¬ í•„í„°
    
    Returns:
        dict: {
            'success': bool,
            'report_path': str,
            'message': str
        }
    """
    from tool_newspaper import generate_section as generate_news_section
    from tool_magazine import (
        generate_section_hackernews,
        generate_section_medium,
        generate_section_books
    )
    from collections import Counter
    
    try:
        print("\n" + "="*60)
        print("ğŸ“Š ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
        print("="*60 + "\n")
        
        # 1. ìš”ì•½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        print("[1/5] ë¸”ë¡œê·¸ ìš”ì•½ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        summaries_result = blog_get_summaries(count=min(count, 100), category=category)
        if not summaries_result.get('success'):
            return {'success': False, 'error': 'ìš”ì•½ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
        
        summaries = summaries_result.get('summaries', [])
        if not summaries:
            return {'success': False, 'error': 'ë¶„ì„í•  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.'}
        
        print(f"      âœ“ {len(summaries)}ê°œ ìš”ì•½ ìˆ˜ì§‘ ì™„ë£Œ\n")
        
        # 2. AIë¡œ ë¸”ë¡œê·¸ ê´€ì‹¬ ì£¼ì œ ì¶”ì¶œ
        print("[2/5] ë¸”ë¡œê·¸ ê´€ì‹¬ ì£¼ì œ ì¶”ì¶œ ì¤‘...")
        summaries_text = ""
        for s in summaries[:30]:
            summaries_text += f"""
---
ì œëª©: {s['title']}
ì¹´í…Œê³ ë¦¬: {s['category']}
í‚¤ì›Œë“œ: {s['keywords']}
ìš”ì•½: {s['summary']}
"""
        
        # AIì—ê²Œ ê´€ì‹¬ ì£¼ì œ ì¶”ì¶œ ìš”ì²­
        topic_prompt = f"""ë‹¤ìŒì€ í•œ ë¸”ë¡œê·¸ì˜ ìµœê·¼ ê¸€ë“¤ ìš”ì•½ì…ë‹ˆë‹¤.

ì´ëŸ° ê¸€ë“¤ì„ ì“´ ì‚¬ëŒì´ ê´€ì‹¬ìˆì–´í•  ë‰´ìŠ¤ ì£¼ì œ 5ê°€ì§€ëŠ”?

êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ ì£¼ì œë¡œ ë‹µí•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: "AI ì—ì´ì „íŠ¸" (O), "ì¸ê³µì§€ëŠ¥" (X, ë„ˆë¬´ ë„“ìŒ)

ì¶œë ¥ í˜•ì‹ (JSONë§Œ, ì„¤ëª… ì—†ì´):
["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3", "ì£¼ì œ4", "ì£¼ì œ5"]

=== ë¸”ë¡œê·¸ ìš”ì•½ ===
{summaries_text}
"""
        
        try:
            ai_result = get_report_ai_client()
            
            if ai_result[1] == "gemini":
                client, _, model_name = ai_result
                response = client.models.generate_content(
                    model=model_name,
                    contents=topic_prompt
                )
                topics_json = response.text.strip()
            elif ai_result[1] == "openai":
                client, _, model_name = ai_result
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": topic_prompt}]
                )
                topics_json = response.choices[0].message.content.strip()
            elif ai_result[1] == "anthropic":
                client, _, model_name = ai_result
                response = client.messages.create(
                    model=model_name,
                    max_tokens=1024,
                    messages=[{"role": "user", "content": topic_prompt}]
                )
                topics_json = response.content[0].text.strip()
            
            # JSON íŒŒì‹±
            import re as regex
            json_match = regex.search(r'\[.*?\]', topics_json, regex.DOTALL)
            if json_match:
                news_keywords = json.loads(json_match.group())
                top_keywords = news_keywords  # í˜¸í™˜ì„± ìœ ì§€
            else:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¹ˆë„ìˆ˜ ê¸°ë°˜ í´ë°±
                all_keywords = []
                for s in summaries:
                    if s.get('keywords'):
                        keywords = [k.strip() for k in s['keywords'].split(',')]
                        all_keywords.extend(keywords)
                keyword_counts = Counter(all_keywords)
                top_keywords = [kw for kw, _ in keyword_counts.most_common(10)]
                news_keywords = top_keywords[:5]
                
        except Exception as e:
            # AI ì‹¤íŒ¨ì‹œ ë¹ˆë„ìˆ˜ ê¸°ë°˜ í´ë°±
            print(f"      âš ï¸ AI ì¶”ì¶œ ì‹¤íŒ¨, ë¹ˆë„ìˆ˜ ë°©ì‹ ì‚¬ìš©: {e}")
            all_keywords = []
            for s in summaries:
                if s.get('keywords'):
                    keywords = [k.strip() for k in s['keywords'].split(',')]
                    all_keywords.extend(keywords)
            keyword_counts = Counter(all_keywords)
            top_keywords = [kw for kw, _ in keyword_counts.most_common(10)]
            news_keywords = top_keywords[:5]
        
        print(f"      âœ“ ê´€ì‹¬ ì£¼ì œ: {', '.join(news_keywords)}\n")
        
        # 3. AIë¡œ ë¶„ì„ + í•™ìŠµ ë°©í–¥ ìƒì„±
        print("[3/5] AI ë¶„ì„ ì¤‘...")
        # summaries_textì— ë‚ ì§œ ì •ë³´ ì¶”ê°€ (2ë‹¨ê³„ì—ì„œ ìƒì„±í•œ ê²ƒ ì¬ì‚¬ìš©)
        summaries_text_with_date = ""
        for s in summaries[:30]:
            summaries_text_with_date += f"""
---
ì œëª©: {s['title']}
ë‚ ì§œ: {s['pub_date']}
ì¹´í…Œê³ ë¦¬: {s['category']}
í‚¤ì›Œë“œ: {s['keywords']}
ìš”ì•½: {s['summary']}
"""
        
        analysis_prompt = f"""ë‹¤ìŒì€ ë¸”ë¡œê·¸ ê¸€ì“´ì´ê°€ ìµœê·¼ ì‘ì„±í•œ ê¸€ë“¤ì˜ ìš”ì•½ì…ë‹ˆë‹¤.

ì´ ê¸€ë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## 1. ë¸”ë¡œê·¸ ë‚´ìš© ë¶„ì„ (ê°„ëµíˆ)
- ì£¼ìš” ê´€ì‹¬ ì£¼ì œ 3~5ê°œ
- ê¸€ì“´ì´ì˜ í•µì‹¬ ë¬¸ì œì˜ì‹
- ìµœê·¼ ê¸€ì˜ íë¦„

## 2. í•™ìŠµ ë°©í–¥ ì œì•ˆ
- í˜„ì¬ ê´€ì‹¬ì‚¬ì™€ ì—°ê²°ë˜ëŠ” ìƒˆë¡œìš´ í•™ìŠµ ì˜ì—­ 3~5ê°œ
- ê° ì˜ì—­ë³„ë¡œ:
  - ì™œ ì´ ì˜ì—­ì´ í•„ìš”í•œì§€ (1~2ë¬¸ì¥)
  - **ë°”ë¡œ ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” ì‘ì€ í”„ë¡œì íŠ¸** 1ê°œ (êµ¬ì²´ì ìœ¼ë¡œ)
  - í•µì‹¬ ì¶”ì²œ ìë£Œ 1ê°œë§Œ (ê°€ì¥ ì¢‹ì€ ì±… ë˜ëŠ” ê°•ì˜ í•˜ë‚˜)

ë¶„ëŸ‰: 1500~2000ì
í†¤: êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸
ì¤‘ìš”: ì¶”ì²œ ìë£ŒëŠ” ê° ì˜ì—­ë‹¹ 1ê°œë§Œ! ì—¬ëŸ¬ ê°œ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ.

=== ë¸”ë¡œê·¸ ìš”ì•½ ë°ì´í„° ===
{summaries_text_with_date}
"""
        
        try:
            ai_result = get_report_ai_client()
            
            if ai_result[1] == "gemini":
                client, _, model_name = ai_result
                response = client.models.generate_content(
                    model=model_name,
                    contents=analysis_prompt
                )
                analysis_content = response.text
            elif ai_result[1] == "openai":
                client, _, model_name = ai_result
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": analysis_prompt}]
                )
                analysis_content = response.choices[0].message.content
            elif ai_result[1] == "anthropic":
                client, _, model_name = ai_result
                response = client.messages.create(
                    model=model_name,
                    max_tokens=4096,
                    messages=[{"role": "user", "content": analysis_prompt}]
                )
                analysis_content = response.content[0].text
                
            print("      âœ“ AI ë¶„ì„ ì™„ë£Œ\n")
                
        except Exception as ai_err:
            analysis_content = f"(AI ë¶„ì„ ì‹¤íŒ¨: {str(ai_err)})"
            print(f"      âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨: {ai_err}\n")
        
        # 4. í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ì„¹ì…˜ ìƒì„±
        print("[4/5] í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        news_sections = ""
        for keyword in news_keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ
            section = generate_news_section(keyword, news_count=3)
            if section.get('success'):
                news_sections += section['markdown']
        print("      âœ“ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ\n")
        
        # 5. í‚¤ì›Œë“œ ê¸°ë°˜ ìë£Œ ì„¹ì…˜ ìƒì„±
        print("[5/5] ê´€ë ¨ ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        resource_sections = ""
        
        # Hacker News
        hn_section = generate_section_hackernews(count=5)
        if hn_section.get('success'):
            resource_sections += hn_section['markdown']
        
        # Medium (ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ)
        for keyword in news_keywords[:2]:
            medium_section = generate_section_medium(topic=keyword, count=2)
            if medium_section.get('success'):
                resource_sections += medium_section['markdown']
        
        # Books (ìƒìœ„ 2ê°œ í‚¤ì›Œë“œ)
        for keyword in news_keywords[:2]:
            books_section = generate_section_books(topic=keyword, count=2)
            if books_section.get('success'):
                resource_sections += books_section['markdown']
        
        # 6. ë§ì¶¤í˜• ë‰´ìŠ¤ ì„¹ì…˜ ìƒì„± (ë‚˜ì˜ ê°œì¸ì •ë³´ ê¸°ë°˜)
        print("[6/6] ë§ì¶¤í˜• ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        personalized_section = ""
        
        profile_path = os.path.join(BASE_DIR, "data", "my_profile.txt")
        if os.path.exists(profile_path):
            try:
                with open(profile_path, 'r', encoding='utf-8') as f:
                    profile_content = f.read().strip()
                
                if profile_content:
                    # AIë¡œ ê°œì¸ì •ë³´ì—ì„œ ì˜¤ëŠ˜ ê´€ì‹¬ìˆì„ ì£¼ì œ ì¶”ì¶œ
                    keyword_prompt = f"""ë‹¤ìŒì€ í•œ ì‚¬ëŒì˜ ê°œì¸ì •ë³´ì…ë‹ˆë‹¤.

ì´ëŸ° ì‚¬ëŒì´ ì˜¤ëŠ˜ ì•„ì¹¨ì— ë‰´ìŠ¤ì—ì„œ í™•ì¸í•˜ê³  ì‹¶ì„ ì£¼ì œ 5ê°€ì§€ëŠ”?

- íˆ¬ì ì¢…ëª© ê´€ë ¨ (ì£¼ê°€, ì‹¤ì , ì „ë§)
- ê°€ì¡±ì´ ìˆëŠ” ì§€ì—­/ë¶„ì•¼ ê´€ë ¨
- ê±°ì£¼ ì§€ì—­ ì†Œì‹
- ì¼ìƒ ê´€ì‹¬ì‚¬ (ì·¨ë¯¸, ë¼ì´í”„ìŠ¤íƒ€ì¼)

ì£¼ì˜: ì§ì—…/ì „ë¬¸ ë¶„ì•¼(ë¬¼ë¦¬í•™, AI ë“±)ëŠ” ì œì™¸ (ë¸”ë¡œê·¸ í‚¤ì›Œë“œë¡œ ì´ë¯¸ ì»¤ë²„ë¨)

ì¶œë ¥ í˜•ì‹ (JSONë§Œ, ì„¤ëª… ì—†ì´):
["ì£¼ì œ1", "ì£¼ì œ2", "ì£¼ì œ3", "ì£¼ì œ4", "ì£¼ì œ5"]

=== ê°œì¸ì •ë³´ ===
{profile_content}
"""
                    
                    try:
                        ai_result = get_report_ai_client()
                        
                        if ai_result[1] == "gemini":
                            client, _, model_name = ai_result
                            response = client.models.generate_content(
                                model=model_name,
                                contents=keyword_prompt
                            )
                            keywords_json = response.text.strip()
                        elif ai_result[1] == "openai":
                            client, _, model_name = ai_result
                            response = client.chat.completions.create(
                                model=model_name,
                                messages=[{"role": "user", "content": keyword_prompt}]
                            )
                            keywords_json = response.choices[0].message.content.strip()
                        elif ai_result[1] == "anthropic":
                            client, _, model_name = ai_result
                            response = client.messages.create(
                                model=model_name,
                                max_tokens=1024,
                                messages=[{"role": "user", "content": keyword_prompt}]
                            )
                            keywords_json = response.content[0].text.strip()
                        
                        # JSON íŒŒì‹±
                        import re as regex
                        json_match = regex.search(r'\[.*?\]', keywords_json, regex.DOTALL)
                        if json_match:
                            personal_keywords = json.loads(json_match.group())
                            print(f"      âœ“ ë§ì¶¤í˜• í‚¤ì›Œë“œ: {', '.join(personal_keywords)}")
                            
                            # ë§ì¶¤í˜• ë‰´ìŠ¤ ìˆ˜ì§‘
                            personal_news_sections = ""
                            for keyword in personal_keywords[:8]:  # ìƒìœ„ 8ê°œ í‚¤ì›Œë“œ (2ë°°)
                                section = generate_news_section(keyword, news_count=6)  # í‚¤ì›Œë“œë‹¹ 6ê°œ (2ë°°)
                                if section.get('success'):
                                    personal_news_sections += section['markdown']
                            
                            # ë§ì¶¤í˜• ìë£Œ ìˆ˜ì§‘
                            personal_resource_sections = ""
                            
                            # Medium (ìƒìœ„ 4ê°œ í‚¤ì›Œë“œ, í‚¤ì›Œë“œë‹¹ 4ê°œ)
                            for keyword in personal_keywords[:4]:
                                medium_section = generate_section_medium(topic=keyword, count=4)
                                if medium_section.get('success'):
                                    personal_resource_sections += medium_section['markdown']
                            
                            # Books (ìƒìœ„ 4ê°œ í‚¤ì›Œë“œ, í‚¤ì›Œë“œë‹¹ 4ê°œ)
                            for keyword in personal_keywords[:4]:
                                books_section = generate_section_books(topic=keyword, count=4)
                                if books_section.get('success'):
                                    personal_resource_sections += books_section['markdown']
                            
                            personalized_section = f"""
---

# ğŸ¯ ë‚˜ë¥¼ ìœ„í•œ ë§ì¶¤í˜• ë‰´ìŠ¤

**ë‚˜ì˜ ê°œì¸ì •ë³´**ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ: `{', '.join(personal_keywords)}`

{personal_news_sections}

## ğŸ“š ë§ì¶¤í˜• ìë£Œ

{personal_resource_sections}
"""
                            print("      âœ“ ë§ì¶¤í˜• ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ\n")
                        else:
                            print("      âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (JSON íŒŒì‹± ì˜¤ë¥˜)\n")
                            
                    except Exception as ai_err:
                        import traceback
                        print(f"      âš ï¸ AI í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {ai_err}")
                        print(f"      ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}\n")
                else:
                    print("      âš ï¸ ê°œì¸ì •ë³´ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤\n")
                    
            except Exception as profile_err:
                print(f"      âš ï¸ ê°œì¸ì •ë³´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {profile_err}\n")
        else:
            print("      âš ï¸ ê°œì¸ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (data/my_profile.txt)\n")
        
        # 7. ìµœê·¼ ê¸€ ì‹¬ì¸µ ë¶„ì„
        print("ğŸ“ ìµœê·¼ ê¸€ ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        latest_post_insight = generate_latest_post_insight(get_report_ai_client)
        print("      âœ“ ìµœê·¼ ê¸€ ë¶„ì„ ì™„ë£Œ\n")
        
        # 8. ì›”ë³„ ê´€ì‹¬ì‚¬ íë¦„ ìƒì„±
        print("ğŸ“… ì›”ë³„ ê´€ì‹¬ì‚¬ íë¦„ ìƒì„± ì¤‘...")
        monthly_summary = generate_monthly_summary(summaries)
        print("      âœ“ ì›”ë³„ íë¦„ ìƒì„± ì™„ë£Œ\n")
        
        # 9. ë³´ê³ ì„œ ì¡°ë¦½
        print("ğŸ“ ë³´ê³ ì„œ ì¡°ë¦½ ì¤‘...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        today_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        
        report_content = f"""# ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ

**ë¶„ì„ ëŒ€ìƒ**: {BLOG_URL}
**ë¶„ì„ ê¸€ ìˆ˜**: {len(summaries)}ê°œ
**ìƒì„± ì¼ì‹œ**: {today_str}
**ì¶”ì¶œ í‚¤ì›Œë“œ**: {', '.join(news_keywords)}

---

{latest_post_insight}

{analysis_content}

---

{monthly_summary}

# ğŸ“° ê´€ë ¨ ë‰´ìŠ¤

í‚¤ì›Œë“œ `{', '.join(news_keywords[:3])}`ë¡œ ê²€ìƒ‰í•œ ìµœì‹  ë‰´ìŠ¤ì…ë‹ˆë‹¤.

{news_sections}

---

# ğŸ“š ê´€ë ¨ ìë£Œ

í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ í•´ì™¸ ìë£Œë“¤ì…ë‹ˆë‹¤.

{resource_sections}
{personalized_section}

---

## ğŸ“Š ë³´ê³ ì„œ ì •ë³´

- **ë¶„ì„ ê¸€ ìˆ˜**: {len(summaries)}ê°œ
- **ì¶”ì¶œ í‚¤ì›Œë“œ**: {', '.join(top_keywords)}
- **ë‰´ìŠ¤ í‚¤ì›Œë“œ**: {', '.join(news_keywords[:3])}
- **ìƒì„± ì‹œê°**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}
"""
        
        # HTML ë³€í™˜ ë° ì €ì¥
        print("ğŸŒ HTML ë³€í™˜ ì¤‘...")
        os.makedirs(OUTPUTS_DIR, exist_ok=True)

        html_content = markdown_to_html(report_content, "ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ", today_str, doc_type="report")
        html_filename = f"blog_insight_report_{timestamp}.html"
        html_filepath = os.path.join(OUTPUTS_DIR, html_filename)

        with open(html_filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"ğŸ’¾ HTML ì €ì¥: {html_filename}")
        print("\n" + "="*60)
        print("âœ… ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
        print("="*60 + "\n")

        return {
            'success': True,
            'report_path': html_filepath,
            'analyzed_count': len(summaries),
            'keywords': news_keywords,
            'message': f'ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {html_filepath}'
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


# =============================================================================
# IndieBiz ë„êµ¬ ì •ì˜ (tools.pyì—ì„œ ì‚¬ìš©)
# ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬ë§Œ (blog_check_new_posts, blog_save_summaryëŠ” í”„ë¡œê·¸ë¨ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì²˜ë¦¬)
# =============================================================================

BLOG_INSIGHT_TOOLS = [
    {
        "name": "blog_get_posts",
        "description": "ë¸”ë¡œê·¸ ê¸€ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. ìš”ì•½ í¬í•¨ ì—¬ë¶€, ìš”ì•½ ì—†ëŠ” ê¸€ë§Œ í•„í„°ë§ ë“± ì˜µì…˜ ì œê³µ.",
        "parameters": {
            "type": "object",
            "properties": {
                "count": {"type": "integer", "description": "ê°€ì ¸ì˜¬ ê¸€ ìˆ˜ (ê¸°ë³¸ 20, ìµœëŒ€ 100)"},
                "offset": {"type": "integer", "description": "ì‹œì‘ ìœ„ì¹˜"},
                "category": {"type": "string", "description": "ì¹´í…Œê³ ë¦¬ í•„í„°"},
                "with_summary": {"type": "boolean", "description": "ìš”ì•½ í¬í•¨ ì—¬ë¶€"},
                "only_without_summary": {"type": "boolean", "description": "ìš”ì•½ ì—†ëŠ” ê¸€ë§Œ ì¡°íšŒ"}
            },
            "required": []
        },
        "function": blog_get_posts
    },
    {
        "name": "blog_get_post",
        "description": "íŠ¹ì • ë¸”ë¡œê·¸ ê¸€ì˜ ì „ì²´ ë‚´ìš©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "post_id": {"type": "string", "description": "í¬ìŠ¤íŠ¸ ID"}
            },
            "required": ["post_id"]
        },
        "function": blog_get_post
    },
    {
        "name": "blog_get_summaries",
        "description": "ì €ì¥ëœ ìš”ì•½ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. ë³´ê³ ì„œ ì‘ì„±ì‹œ í™œìš©.",
        "parameters": {
            "type": "object",
            "properties": {
                "count": {"type": "integer", "description": "ê°€ì ¸ì˜¬ ê°œìˆ˜ (ê¸°ë³¸ 20, ìµœëŒ€ 100)"},
                "offset": {"type": "integer", "description": "ì‹œì‘ ìœ„ì¹˜"},
                "category": {"type": "string", "description": "ì¹´í…Œê³ ë¦¬ í•„í„°"}
            },
            "required": []
        },
        "function": blog_get_summaries
    },
    {
        "name": "blog_search",
        "description": "í‚¤ì›Œë“œë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "ê²€ìƒ‰ì–´"},
                "count": {"type": "integer", "description": "ê²°ê³¼ ê°œìˆ˜ (ê¸°ë³¸ 20, ìµœëŒ€ 50)"},
                "search_in": {"type": "string", "enum": ["title", "content", "all"], "description": "ê²€ìƒ‰ ë²”ìœ„"}
            },
            "required": ["query"]
        },
        "function": blog_search
    },
    {
        "name": "blog_stats",
        "description": "ë¸”ë¡œê·¸ DB í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ì „ì²´ ê¸€ ìˆ˜, ìš”ì•½ ìˆ˜, ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ë“±.",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": []
        },
        "function": blog_stats
    },
    {
        "name": "blog_insight_report",
        "description": "ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ í†µí•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ ë¶„ì„ + í•™ìŠµ ë°©í–¥ ì œì•ˆ + í‚¤ì›Œë“œ ê¸°ë°˜ ì‹ ë¬¸ + í‚¤ì›Œë“œ ê¸°ë°˜ ì¡ì§€ê°€ í¬í•¨ëœ ì¢…í•© ë³´ê³ ì„œì…ë‹ˆë‹¤.",
        "uses_ai": True,
        "ai_config_key": "blog_insight",
        "parameters": {
            "type": "object",
            "properties": {
                "count": {"type": "integer", "description": "ë¶„ì„í•  ê¸€ ìˆ˜ (ê¸°ë³¸ 50, ìµœëŒ€ 100)"},
                "category": {"type": "string", "description": "ì¹´í…Œê³ ë¦¬ í•„í„° (ì„ íƒ)"}
            },
            "required": []
        },
        "function": blog_insight_report
    }
]


# í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    print("=== Blog Insight Tool Test ===\n")
    
    # ìƒˆ ê¸€ í™•ì¸
    print("1. Checking new posts...")
    result = blog_check_new_posts()
    print(f"   New: {result.get('new_count', 0)}, Total: {result.get('total_posts', 0)}")
    
    # í†µê³„
    print("\n2. Stats...")
    stats = blog_stats()
    print(f"   Posts: {stats.get('total_posts')}")
    print(f"   Summaries: {stats.get('total_summaries')}")
    print(f"   Without summary: {stats.get('posts_without_summary')}")
    
    # ê¸€ ëª©ë¡
    print("\n3. Recent posts...")
    posts = blog_get_posts(count=3)
    for p in posts.get('posts', []):
        print(f"   - {p['title'][:40]}...")
    
    print("\nDone!")
