"""
Blog Insight Tool for IndieBiz
==============================
ë¸”ë¡œê·¸ ê¸€ì„ ìˆ˜ì§‘í•˜ê³  AI ìš”ì•½ì„ ì €ì¥í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë„êµ¬

ì„¤ê³„ ì›ì¹™:
- ê¸°ì´ˆ ë„êµ¬ë§Œ ì œê³µ, AI ì—ì´ì „íŠ¸ê°€ ì¡°í•©í•´ì„œ ì‚¬ìš©
- 500ì ìš”ì•½ì€ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥
- ë³´ê³ ì„œ ìƒì„±ì€ ë‚´ë¶€ AIê°€ ì²˜ë¦¬ (í† í° ì ˆì•½)
"""

import os
import sys
import json
import sqlite3
import requests
import re
import feedparser
from datetime import datetime
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup

# common ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
_backend_dir = os.path.join(os.path.dirname(__file__), "..", "..", "..", "..", "..", "backend")
if _backend_dir not in sys.path:
    sys.path.insert(0, os.path.abspath(_backend_dir))

from common.html_utils import clean_html

# ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
DB_PATH = os.path.join(DATA_DIR, "blog_insight.db")

# ì‹œìŠ¤í…œ AI ì„¤ì • ê²½ë¡œ
BACKEND_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "..", "..", "..", "backend"))
SYSTEM_AI_CONFIG_PATH = os.path.join(BACKEND_DIR, "data", "system_ai_config.json")

BLOG_URL = "https://irepublic.tistory.com"
RSS_URL = f"{BLOG_URL}/rss"
RSS_SIZE = 50


def markdown_to_html(content: str, title: str, date_str: str, doc_type: str = "report") -> str:
    """ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜"""
    try:
        import markdown
        html_body = markdown.markdown(content, extensions=['tables', 'fenced_code'])
    except ImportError:
        html_body = f"<pre style='white-space: pre-wrap;'>{content}</pre>"

    return f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;700&display=swap');
        body {{ font-family: 'Noto Sans KR', sans-serif; max-width: 900px; margin: 0 auto; padding: 40px 20px; line-height: 1.8; color: #333; background-color: #f9f9f9; }}
        .content-card {{ background: white; padding: 40px; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.08); }}
        h1 {{ color: #1a2a6c; border-bottom: 3px solid #1a2a6c; padding-bottom: 10px; font-size: 2.5em; }}
        h2 {{ color: #1a2a6c; margin-top: 1.5em; border-left: 5px solid #1a2a6c; padding-left: 15px; }}
        h3 {{ color: #2c3e50; margin-top: 1.2em; }}
        a {{ color: #3498db; text-decoration: none; border-bottom: 1px solid #3498db; }}
        a:hover {{ color: #2980b9; border-bottom-width: 2px; }}
        pre {{ background: #2c3e50; color: #ecf0f1; padding: 15px; border-radius: 8px; overflow-x: auto; }}
        code {{ font-family: 'Courier New', Courier, monospace; background: #f0f0f0; padding: 2px 5px; border-radius: 3px; }}
        blockquote {{ margin: 20px 0; padding: 10px 20px; border-left: 5px solid #ddd; font-style: italic; color: #666; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .meta {{ color: #7f8c8d; font-size: 0.9em; margin-bottom: 30px; }}
        hr {{ border: 0; border-top: 1px solid #eee; margin: 40px 0; }}
        @media print {{ body {{ background: white; }} .content-card {{ box-shadow: none; padding: 0; }} }}
    </style>
</head>
<body>
    <div class="content-card">
        <h1>{title}</h1>
        <p class="meta">ë°œí–‰ì¼: {date_str} | ì¶œì²˜: {BLOG_URL}</p>
        {html_body}
    </div>
</body>
</html>"""


def load_system_ai_config() -> dict:
    """ì‹œìŠ¤í…œ AI ì„¤ì • ë¡œë“œ"""
    if os.path.exists(SYSTEM_AI_CONFIG_PATH):
        try:
            with open(SYSTEM_AI_CONFIG_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        "provider": "google",
        "model": "gemini-2.0-flash",
        "apiKey": ""
    }


def get_report_ai_client():
    """ë³´ê³ ì„œ ìƒì„±ìš© AI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    config = load_system_ai_config()
    provider = config.get("provider", "google")
    model = config.get("model", "gemini-2.0-flash")
    api_key = config.get("apiKey") or config.get("api_key", "")

    if provider in ["google", "gemini"]:
        from google import genai
        client = genai.Client(api_key=api_key)
        return client, "gemini", model
    elif provider == "openai":
        import openai
        client = openai.OpenAI(api_key=api_key)
        return client, "openai", model
    elif provider == "anthropic":
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        return client, "anthropic", model
    else:
        raise ValueError(f"Unknown provider: {provider}")


def get_db() -> sqlite3.Connection:
    """DB ì—°ê²° ë° í…Œì´ë¸” ìƒì„±"""
    os.makedirs(DATA_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row

    conn.execute("""
        CREATE TABLE IF NOT EXISTS posts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE,
            title TEXT,
            category TEXT,
            pub_date DATETIME,
            content TEXT,
            char_count INTEGER,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)

    conn.execute("""
        CREATE TABLE IF NOT EXISTS summaries (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id TEXT UNIQUE,
            summary TEXT,
            keywords TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (post_id) REFERENCES posts(post_id)
        )
    """)

    # FTS5 ì „ë¬¸ ê²€ìƒ‰ ê°€ìƒ í…Œì´ë¸” (BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
    conn.execute("""
        CREATE VIRTUAL TABLE IF NOT EXISTS posts_fts USING fts5(
            title, content,
            content='posts', content_rowid='id'
        )
    """)

    # FTS5 ìë™ ë™ê¸°í™” íŠ¸ë¦¬ê±°
    conn.execute("""
        CREATE TRIGGER IF NOT EXISTS posts_fts_insert
        AFTER INSERT ON posts BEGIN
            INSERT INTO posts_fts(rowid, title, content)
            VALUES (new.id, new.title, new.content);
        END
    """)
    conn.execute("""
        CREATE TRIGGER IF NOT EXISTS posts_fts_delete
        AFTER DELETE ON posts BEGIN
            INSERT INTO posts_fts(posts_fts, rowid, title, content)
            VALUES ('delete', old.id, old.title, old.content);
        END
    """)
    conn.execute("""
        CREATE TRIGGER IF NOT EXISTS posts_fts_update
        AFTER UPDATE ON posts BEGIN
            INSERT INTO posts_fts(posts_fts, rowid, title, content)
            VALUES ('delete', old.id, old.title, old.content);
            INSERT INTO posts_fts(rowid, title, content)
            VALUES (new.id, new.title, new.content);
        END
    """)

    # RAG ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒíƒœ ì¶”ì  í…Œì´ë¸”
    conn.execute("""
        CREATE TABLE IF NOT EXISTS search_index_status (
            post_id TEXT PRIMARY KEY,
            indexed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            embedding_version TEXT DEFAULT 'v1',
            FOREIGN KEY (post_id) REFERENCES posts(post_id)
        )
    """)

    conn.commit()

    # ê¸°ì¡´ í¬ìŠ¤íŠ¸ë¥¼ FTS5ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (ìµœì´ˆ 1íšŒ)
    _migrate_fts5_if_needed(conn)

    return conn


def _migrate_fts5_if_needed(conn: sqlite3.Connection):
    """ê¸°ì¡´ í¬ìŠ¤íŠ¸ë¥¼ FTS5 ì¸ë±ìŠ¤ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ (ìµœì´ˆ 1íšŒ)"""
    try:
        fts_count = conn.execute("SELECT COUNT(*) FROM posts_fts").fetchone()[0]
        posts_count = conn.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
        if fts_count >= posts_count:
            return  # ì´ë¯¸ ë™ê¸°í™”ë¨
        print(f"[Blog] FTS5 ì¸ë±ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘... ({posts_count}ê°œ í¬ìŠ¤íŠ¸)")
        conn.execute("INSERT INTO posts_fts(posts_fts) VALUES('rebuild')")
        conn.commit()
        print(f"[Blog] FTS5 ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
    except Exception as e:
        print(f"[Blog] FTS5 ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í‚µ: {e}")


def parse_rss_date(rss_date: str) -> Optional[str]:
    try:
        dt = datetime.strptime(rss_date.strip(), "%a, %d %b %Y %H:%M:%S %z")
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except:
        return None


# strip_htmlì€ common.html_utils.clean_htmlë¡œ ëŒ€ì²´
strip_html = clean_html


def fetch_rss_feed() -> List[Dict[str, Any]]:
    url = f"{RSS_URL}?size={RSS_SIZE}"
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.content, 'xml')
    items = soup.find_all('item')
    
    posts = []
    for item in items:
        link = item.find('link').text if item.find('link') else ""
        post_id_match = re.search(r'/(\d+)$', link)
        post_id = post_id_match.group(1) if post_id_match else link
        
        title = item.find('title').text if item.find('title') else ""
        pub_date = item.find('pubDate').text if item.find('pubDate') else ""
        description = item.find('description').text if item.find('description') else ""
        
        categories = [cat.text for cat in item.find_all('category')]
        category = categories[0] if categories else ""
        
        content_text = strip_html(description)
        
        posts.append({
            'post_id': post_id,
            'title': title,
            'link': link,
            'pub_date': pub_date,
            'category': category,
            'content': content_text
        })
    
    return posts


# =============================================================================
# ë„êµ¬ í•¨ìˆ˜ë“¤
# =============================================================================

def blog_check_new_posts() -> Dict[str, Any]:
    try:
        conn = get_db()
        existing = set(row[0] for row in conn.execute("SELECT post_id FROM posts").fetchall())
        rss_posts = fetch_rss_feed()
        
        new_posts = []
        for post in rss_posts:
            if post['post_id'] not in existing:
                pub_date_db = parse_rss_date(post['pub_date'])
                conn.execute("""
                    INSERT OR IGNORE INTO posts (post_id, title, category, pub_date, content, char_count)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (post['post_id'], post['title'], post['category'], pub_date_db, post['content'], len(post['content'])))
                new_posts.append({'post_id': post['post_id'], 'title': post['title'], 'pub_date': pub_date_db})
        
        conn.commit()
        total = conn.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
        conn.close()

        # ìƒˆ í¬ìŠ¤íŠ¸ì˜ ë²¡í„° ì„ë² ë”© ìƒì„± (ì„ íƒì , ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
        if new_posts:
            try:
                from tool_blog_rag import BlogHybridSearch
                engine = BlogHybridSearch()
                indexed = engine.index_new_posts()
                if indexed > 0:
                    print(f"[Blog] {indexed}ê°œ ìƒˆ í¬ìŠ¤íŠ¸ RAG ì¸ë±ì‹± ì™„ë£Œ")
            except Exception as e:
                print(f"[Blog] RAG ì¸ë±ì‹± ìŠ¤í‚µ: {e}")

        return {'success': True, 'new_count': len(new_posts), 'new_posts': new_posts, 'total_posts': total}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_posts(count: int = 20, offset: int = 0, category: Optional[str] = None, with_summary: bool = False, only_without_summary: bool = False) -> Dict[str, Any]:
    try:
        conn = get_db()
        count = min(count, 100)
        
        if only_without_summary:
            query = "SELECT p.* FROM posts p LEFT JOIN summaries s ON p.post_id = s.post_id WHERE s.post_id IS NULL"
            params = []
        else:
            query = "SELECT p.* " + (", s.summary, s.keywords" if with_summary else "") + " FROM posts p " + ("LEFT JOIN summaries s ON p.post_id = s.post_id" if with_summary else "") + " WHERE 1=1"
            params = []
        
        if category:
            query += " AND p.category = ?"
            params.append(category)
        
        query += " ORDER BY p.pub_date DESC LIMIT ? OFFSET ?"
        params.extend([count, offset])
        
        rows = conn.execute(query, params).fetchall()
        posts = []
        for row in rows:
            post = {'post_id': row['post_id'], 'title': row['title'], 'category': row['category'], 'pub_date': row['pub_date'], 'content_preview': row['content'][:300] + '...'}
            if with_summary and 'summary' in row.keys():
                post['summary'] = row['summary']
                post['keywords'] = row['keywords']
            posts.append(post)
        
        conn.close()
        return {'success': True, 'count': len(posts), 'posts': posts}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_post(post_id: str) -> Dict[str, Any]:
    try:
        conn = get_db()
        row = conn.execute("SELECT p.*, s.summary, s.keywords FROM posts p LEFT JOIN summaries s ON p.post_id = s.post_id WHERE p.post_id = ?", (post_id,)).fetchone()
        conn.close()
        if not row: return {'success': False, 'error': f'Post not found: {post_id}'}
        return {'success': True, 'post': {'post_id': row['post_id'], 'title': row['title'], 'category': row['category'], 'pub_date': row['pub_date'], 'content': row['content'], 'summary': row['summary'], 'keywords': row['keywords'], 'link': f"{BLOG_URL}/{row['post_id']}"}}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_get_summaries(count: int = 20, offset: int = 0, category: Optional[str] = None) -> Dict[str, Any]:
    try:
        conn = get_db()
        count = min(count, 100)
        query = "SELECT p.post_id, p.title, p.category, p.pub_date, s.summary, s.keywords FROM summaries s JOIN posts p ON s.post_id = p.post_id WHERE 1=1"
        params = []
        if category: query += " AND p.category = ?"; params.append(category)
        query += " ORDER BY p.pub_date DESC LIMIT ? OFFSET ?"
        params.extend([count, offset])
        rows = conn.execute(query, params).fetchall()
        summaries = [dict(row) for row in rows]
        conn.close()
        return {'success': True, 'count': len(summaries), 'summaries': summaries}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_save_summary(post_id: str, summary: str, keywords: str = "") -> Dict[str, Any]:
    try:
        conn = get_db()
        conn.execute("INSERT OR REPLACE INTO summaries (post_id, summary, keywords) VALUES (?, ?, ?)", (post_id, summary, keywords))
        conn.commit()
        conn.close()
        return {'success': True}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_search(query: str, count: int = 20, search_in: str = "all") -> Dict[str, Any]:
    try:
        conn = get_db()
        count = min(count, 50)
        where = "p.title LIKE ?" if search_in == "title" else "p.content LIKE ?" if search_in == "content" else "(p.title LIKE ? OR p.content LIKE ?)"
        like_query = f"%{query}%"
        params = [like_query, like_query, count] if search_in == "all" else [like_query, count]
        sql = f"SELECT p.*, s.summary FROM posts p LEFT JOIN summaries s ON p.post_id = s.post_id WHERE {where} ORDER BY p.pub_date DESC LIMIT ?"
        rows = conn.execute(sql, params).fetchall()
        results = [{'post_id': row['post_id'], 'title': row['title'], 'category': row['category'], 'pub_date': row['pub_date'], 'content_preview': row['content'][:200] + '...', 'has_summary': row['summary'] is not None} for row in rows]
        conn.close()
        return {'success': True, 'results': results}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def blog_stats() -> Dict[str, Any]:
    try:
        conn = get_db()
        total_posts = conn.execute("SELECT COUNT(*) FROM posts").fetchone()[0]
        total_summaries = conn.execute("SELECT COUNT(*) FROM summaries").fetchone()[0]
        without_summary = conn.execute("SELECT COUNT(*) FROM posts p LEFT JOIN summaries s ON p.post_id = s.post_id WHERE s.post_id IS NULL").fetchone()[0]
        conn.close()
        return {'success': True, 'total_posts': total_posts, 'total_summaries': total_summaries, 'posts_without_summary': without_summary}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def _search_news(keyword: str, max_results: int = 5) -> List[Dict]:
    results = []
    try:
        from duckduckgo_search import DDGS
        with DDGS() as ddgs:
            for r in ddgs.news(keyword, max_results=max_results):
                results.append({"title": r['title'], "snippet": r['body'], "link": r['url'], "source": r['source']})
    except: pass
    return results


def _fetch_rss(url: str, limit: int = 5) -> List[Dict]:
    try:
        feed = feedparser.parse(url)
        return [{"title": e.title, "link": e.link, "snippet": strip_html(e.summary)[:200]} for e in feed.entries[:limit]]
    except: return []


def blog_insight_report(count: int = 50, category: Optional[str] = None, project_path: str = ".") -> Dict[str, Any]:
    """ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ í†µí•© ë³´ê³ ì„œ ìƒì„± (ë™ê¸°í™” í¬í•¨)"""
    try:
        # 0. ë°ì´í„° ë™ê¸°í™”
        print("ğŸ”„ ë¸”ë¡œê·¸ ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì¤‘...")
        blog_check_new_posts()
        
        # 1. ìš”ì•½ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        summaries_result = blog_get_summaries(count=count, category=category)
        summaries = summaries_result.get('summaries', [])
        
        # ìš”ì•½ì´ ë¶€ì¡±í•˜ë©´ ë¯¸ìš”ì•½ ê¸€ì— ëŒ€í•´ ì¦‰ì„ ìš”ì•½ ìƒì„± (ìµœëŒ€ 5ê°œ)
        if len(summaries) < 10:
            print("ğŸ“ ë¯¸ìš”ì•½ ê¸€ ìš”ì•½ ìƒì„± ì¤‘...")
            conn = get_db()
            missing = conn.execute("SELECT p.post_id, p.title, p.content FROM posts p LEFT JOIN summaries s ON p.post_id = s.post_id WHERE s.post_id IS NULL ORDER BY p.pub_date DESC LIMIT 5").fetchall()
            if missing:
                client, provider, model = get_report_ai_client()
                for m in missing:
                    prompt = f"ë‹¤ìŒ ë¸”ë¡œê·¸ ê¸€ì„ 500ì ë‚´ì™¸ë¡œ ìš”ì•½í•˜ê³  í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•´ì¤˜. í˜•ì‹: ìš”ì•½ë‚´ìš© | í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2,í‚¤ì›Œë“œ3\n\nì œëª©: {m['title']}\në‚´ìš©: {m['content'][:2000]}"
                    try:
                        if provider == "gemini": res = client.models.generate_content(model=model, contents=prompt).text
                        elif provider == "openai": res = client.chat.completions.create(model=model, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
                        if "|" in res:
                            summ, kws = res.split("|", 1)
                            blog_save_summary(m['post_id'], summ.strip(), kws.strip())
                    except: pass
                # ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
                summaries = blog_get_summaries(count=count, category=category).get('summaries', [])
        
        if not summaries: return {'success': False, 'error': 'ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}

        # 2. AI ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        print("ğŸ§  AI ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘...")
        client, provider, model = get_report_ai_client()
        context = "\n".join([f"- {s['title']} ({s['category']}): {s['summary'][:100]}..." for s in summaries[:20]])
        prompt = f"ë‹¤ìŒ ë¸”ë¡œê·¸ ìš”ì•½ë“¤ì„ ë¶„ì„í•˜ì—¬ 1.ìµœê·¼ ê´€ì‹¬ì‚¬ ë¶„ì„, 2.í•™ìŠµ ë°©í–¥ ì œì•ˆ(3ê°€ì§€ êµ¬ì²´ì  ì˜ì—­ê³¼ í”„ë¡œì íŠ¸), 3.ê²€ìƒ‰í•  ë‰´ìŠ¤ í‚¤ì›Œë“œ 3ê°œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.\n\n{context}"
        
        if provider == "gemini": analysis = client.models.generate_content(model=model, contents=prompt).text
        elif provider == "openai": analysis = client.chat.completions.create(model=model, messages=[{"role": "user", "content": prompt}]).choices[0].message.content
        
        # ë‰´ìŠ¤ í‚¤ì›Œë“œ íŒŒì‹± (ê°„ë‹¨íˆ)
        news_keywords = ["AI", "ê¸°ìˆ ", "ë¹„ì¦ˆë‹ˆìŠ¤"]
        for line in analysis.split("\n"):
            if "ë‰´ìŠ¤ í‚¤ì›Œë“œ" in line or "í‚¤ì›Œë“œ:" in line:
                found = re.findall(r'["\']([^"\']+)["\']|`([^`]+)`', line)
                if found: news_keywords = [f[0] or f[1] for f in found]
        
        # 3. ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
        print("ğŸŒ ê´€ë ¨ ë‰´ìŠ¤ ë° ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        news_section = "\n## ğŸ“° ê´€ë ¨ ë‰´ìŠ¤\n"
        for kw in news_keywords[:3]:
            for art in _search_news(kw, 3):
                news_section += f"- [{art['title']}]({art['link']}) ({art['source']})\n"
        
        # 4. ë³´ê³ ì„œ ì¡°ë¦½ ë° ì €ì¥
        report_md = f"# ğŸ“Š ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ\n\n{analysis}\n\n{news_section}\n\n---\n*ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        html_content = markdown_to_html(report_md, "ë¸”ë¡œê·¸ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ", datetime.now().strftime("%Y-%m-%d"))
        
        out_dir = os.path.join(project_path, "outputs")
        os.makedirs(out_dir, exist_ok=True)
        filename = f"blog_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        filepath = os.path.join(out_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return {'success': True, 'report_path': os.path.abspath(filepath), 'message': f'ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {filepath}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}
